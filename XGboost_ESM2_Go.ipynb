{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d09e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import os, re, shutil, sys\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FASTA = \"train_sequences.fasta\"\n",
    "TRAIN_TERMS = \"train_terms.tsv\"\n",
    "TRAIN_TAXONOMY = \"train_taxonomy.tsv\"\n",
    "TEST_FASTA  = \"testsuperset.fasta\"\n",
    "TEST_TAXON  = \"testsuperset-taxon-list.tsv\"\n",
    "IA_TSV      = \"IA.tsv\"\n",
    "TRAIN_EMB= \"train_embeddings.npy\"\n",
    "TEST_EMB = \"test_embeddings.npy\"\n",
    "OUT_SUBMISSION = \"submission.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08b09bc",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "Data Loading\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97590c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training FASTA...\n",
      "Loading training terms...\n",
      "Training instances used: 82404\n",
      "Kept by freq: 1200 | Allowed (IA): 40121\n",
      "Kept after whitelist: 1200\n"
     ]
    }
   ],
   "source": [
    "# Label space control (important for runtime)\n",
    "MIN_COUNT     = 50       # keep GO terms appearing >= 2 times\n",
    "MAX_TERMS     = 1200    # cap number of GO classes (raise if your session allows)\n",
    "\n",
    "# Emission control (be generous; evaluator picks the threshold)\n",
    "MIN_PROB      = 1e-6\n",
    "TOP_K         = 200\n",
    "CAP_PER_PROT  = 1500\n",
    "\n",
    "\n",
    "\n",
    "# Training loop controls\n",
    "MIN_POSITIVES_PER_CLASS   = 5            # skip classes with too few positives\n",
    "VAL_SIZE                  = 0.1          # class-wise validation split for early stopping\n",
    "RANDOM_STATE              = 42\n",
    "\n",
    "\n",
    "# Read protein IDs and amino acid sequences from a FASTA file\n",
    "def read_fasta_ids_and_seqs(path):\n",
    "    \n",
    "    \n",
    "    ids, seqs, cur_id, cur_seq = [], [], None, []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                if cur_id is not None:\n",
    "                    ids.append(cur_id); seqs.append(\"\".join(cur_seq))\n",
    "                header = line[1:].strip()\n",
    "                cur_id = header.split(\"|\")[1].split()[0] if \"|\" in header else header.split()[0]\n",
    "                cur_seq = []\n",
    "            else:\n",
    "                cur_seq.append(line.strip())\n",
    "        if cur_id is not None:\n",
    "            ids.append(cur_id); seqs.append(\"\".join(cur_seq))\n",
    "    return ids, seqs\n",
    "\n",
    "# Load training GO annotations and standardize column names\n",
    "def load_train_terms(path):\n",
    "    df0 = pd.read_csv(path, sep=\"\\t\", dtype=str)\n",
    "    cols = [c.lower() for c in df0.columns]; df0.columns = cols\n",
    "    if \"entryid\" in cols and \"term\" in cols:\n",
    "        df = df0.rename(columns={\"entryid\":\"protein_id\",\"term\":\"go_id\"})[[\"protein_id\",\"go_id\"]]\n",
    "    else:\n",
    "        df = df0.iloc[:, :2].copy()\n",
    "        df.columns = [\"protein_id\",\"go_id\"]\n",
    "    return df.dropna().drop_duplicates()\n",
    "\n",
    "# Load protein taxonomy information as a dictionary (protein_id -> taxon)\n",
    "def load_taxonomy(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, dtype=str)\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(\"taxonomy TSV must have at least two columns.\")\n",
    "    df = df.iloc[:, :2].copy()\n",
    "    df.columns = [\"protein_id\",\"taxon\"]\n",
    "    return dict(zip(df[\"protein_id\"], df[\"taxon\"]))\n",
    "\n",
    "# Build a mapping from protein IDs to their associated GO term sets\n",
    "def build_label_sets(df_terms):\n",
    "    mp = defaultdict(set)\n",
    "    for p,g in df_terms[[\"protein_id\",\"go_id\"]].itertuples(index=False):\n",
    "        mp[p].add(g)\n",
    "    return mp\n",
    "\n",
    "# Filter GO terms by frequency and optionally limit the total number of terms\n",
    "def filter_terms_by_freq(p2t, min_count=2, max_terms=None):\n",
    "    c = Counter()\n",
    "    for ts in p2t.values(): c.update(ts)\n",
    "    items = [(g,n) for g,n in c.items() if n >= min_count]\n",
    "    items.sort(key=lambda x: x[1], reverse=True)\n",
    "    if max_terms: items = items[:max_terms]\n",
    "    return set(g for g,_ in items)\n",
    "\n",
    "# Load the whitelist of allowed GO terms from an IA TSV file\n",
    "def load_allowed_go_terms(ia_path):\n",
    "    df = pd.read_csv(ia_path, sep=\"\\t\", dtype=str)\n",
    "    go_cols = [c for c in df.columns if \"go\" in c.lower() or \"term\" in c.lower()]\n",
    "    if go_cols:\n",
    "        vals = df[go_cols[0]].dropna().astype(str).tolist()\n",
    "        return {v for v in vals if v.startswith(\"GO:\")}\n",
    "    s = set()\n",
    "    for col in df.columns:\n",
    "        s |= set(df[col].dropna().astype(str))\n",
    "    return {x for x in s if x.startswith(\"GO:\")}\n",
    "\n",
    "# Format prediction scores to a fixed number of significant digits\n",
    "def round_sig(x: float, sig=3) -> str:\n",
    "    if x <= 0: return None\n",
    "    s = f\"{x:.{sig}g}\"\n",
    "    return s if float(s) > 0 else \"0.001\"\n",
    "\n",
    "\n",
    "\n",
    "def get_xgb_device_params():\n",
    "    # Use GPU if available; else CPU\n",
    "    try:\n",
    "        import subprocess\n",
    "        has_gpu = False\n",
    "        try:\n",
    "            out = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "            has_gpu = (out.returncode == 0)\n",
    "        except Exception:\n",
    "            has_gpu = False\n",
    "        if has_gpu:\n",
    "            return dict(tree_method=\"gpu_hist\", predictor=\"gpu_predictor\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return dict(tree_method=\"hist\", predictor=\"auto\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading training FASTA...\")\n",
    "train_ids, train_seqs = read_fasta_ids_and_seqs(TRAIN_FASTA)\n",
    "print(\"Loading training terms...\")\n",
    "terms_df = load_train_terms(TRAIN_TERMS)\n",
    "prot2terms_all = build_label_sets(terms_df)\n",
    "\n",
    "# Keep only proteins with labels and present in FASTA\n",
    "X_text, y_terms, used_ids = [], [], []\n",
    "for pid, seq in zip(train_ids, train_seqs):\n",
    "    t = prot2terms_all.get(pid)\n",
    "    if t:\n",
    "        used_ids.append(pid); X_text.append(seq); y_terms.append(sorted(t))\n",
    "print(\"Training instances used:\", len(used_ids))\n",
    "\n",
    "\n",
    "\n",
    "# Term filtering (frequency) and whitelist (IA.tsv)\n",
    "kept_freq = filter_terms_by_freq({pid:set(ts) for pid,ts in zip(used_ids, y_terms)},\n",
    "                                 min_count=MIN_COUNT, max_terms=MAX_TERMS)\n",
    "allowed_terms = load_allowed_go_terms(IA_TSV)\n",
    "print(\"Kept by freq:\", len(kept_freq), \"| Allowed (IA):\", len(allowed_terms))\n",
    "kept_terms = kept_freq & allowed_terms\n",
    "print(\"Kept after whitelist:\", len(kept_terms))\n",
    "y_filtered = [[t for t in ts if t in kept_terms] for ts in y_terms]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7808efbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed embeddings...\n",
      "X_test shape: (224309, 1280)\n"
     ]
    }
   ],
   "source": [
    "#Loading Embedding Features\n",
    "X_train = np.load(TRAIN_EMB)  # shape (N_train, D)\n",
    "X_test  = np.load(TEST_EMB)   # shape (N_test, D)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21a930e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (82404, 1280)\n",
      "Label matrix shape: (82404, 1200)\n",
      "Num GO classes: 1200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "all_terms_sorted = sorted(list(kept_terms))\n",
    "mlb = MultiLabelBinarizer(classes=all_terms_sorted)\n",
    "Y = mlb.fit_transform(y_filtered)\n",
    "num_classes = Y.shape[1]\n",
    "\n",
    "\n",
    "print(\"Embedding shape:\", X_train.shape)\n",
    "print(\"Label matrix shape:\", Y.shape)\n",
    "print(\"Num GO classes:\", len(all_terms_sorted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d677c98",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "Training\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8542925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, Y, test_size=VAL_SIZE,\n",
    "    random_state=RANDOM_STATE, stratify=(Y.sum(axis=1) > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5029548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost for each GO term...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training XGB:   0%|          | 1/1200 [00:04<1:36:28,  4.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_metric\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_params\n\u001b[0;32m     29\u001b[0m }\n\u001b[0;32m     31\u001b[0m bst \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m     32\u001b[0m     params,\n\u001b[0;32m     33\u001b[0m     dtrain,\n\u001b[0;32m     34\u001b[0m     num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[0;32m     35\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:885\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 885\u001b[0m handle, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_data_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnthread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_split_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_split_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m handle\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\data.py:1382\u001b[0m, in \u001b[0;36mdispatch_data_backend\u001b[1;34m(data, missing, threads, feature_names, feature_types, enable_categorical, data_split_mode)\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_scipy_csr(\n\u001b[0;32m   1374\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mtocsr(),\n\u001b[0;32m   1375\u001b[0m         missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1379\u001b[0m         data_split_mode\u001b[38;5;241m=\u001b[39mdata_split_mode,\n\u001b[0;32m   1380\u001b[0m     )\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_np_array_like(data):\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_from_numpy_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_split_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_split_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_uri(data):\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_uri(data, missing, feature_names, feature_types, data_split_mode)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\data.py:272\u001b[0m, in \u001b[0;36m_from_numpy_array\u001b[1;34m(data, missing, nthread, feature_names, feature_types, data_split_mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m data, _ \u001b[38;5;241m=\u001b[39m _ensure_np_dtype(data, data\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    270\u001b[0m handle \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_void_p()\n\u001b[0;32m    271\u001b[0m _check_call(\n\u001b[1;32m--> 272\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGDMatrixCreateFromDense\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray_interface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmake_jcargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnthread\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_split_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_split_mode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m )\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handle, feature_names, feature_types\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "device_params = {\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"predictor\": \"gpu_predictor\"\n",
    "}\n",
    "models = []\n",
    "\n",
    "print(\"Training XGBoost for each GO term...\")\n",
    "\n",
    "for i in tqdm(range(num_classes), desc=\"Training XGB\"):\n",
    "    y_i = y_tr[:, i]\n",
    "\n",
    "    # Skip if class has no positives\n",
    "    if y_i.sum() == 0:\n",
    "        models.append(None)\n",
    "        continue\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_i)\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"max_depth\": 6,\n",
    "        \"eta\": 0.1,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        **device_params\n",
    "    }\n",
    "\n",
    "    bst = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=200\n",
    "    )\n",
    "\n",
    "    models.append(bst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258a180",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "Hierarchy Constraints</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f04057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse GO hierarchy from an OBO file and build a mapping from each GO term to its parent terms\n",
    "def load_go_parents(obo_file):\n",
    "    parents_map = {}\n",
    "    current_term = None\n",
    "\n",
    "    with open(obo_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line == \"[Term]\":\n",
    "                current_term = None\n",
    "\n",
    "            elif line.startswith(\"id: GO:\"):\n",
    "                current_term = line.split(\"id: \")[1]\n",
    "                parents_map[current_term] = set()\n",
    "\n",
    "            elif line.startswith(\"is_a:\") and current_term:\n",
    "                parent = line.split(\"is_a: \")[1].split(\" !\")[0]\n",
    "                parents_map[current_term].add(parent)\n",
    "\n",
    "    return parents_map\n",
    "parents_map = load_go_parents(\"go-basic.obo\")\n",
    "\n",
    "# Restrict GO parent relationships to the set of GO terms used by the model\n",
    "def restrict_go_parents(parents_map, classes):\n",
    "    term_to_idx = {t: i for i, t in enumerate(classes)}\n",
    "\n",
    "    restricted_parents = {\n",
    "        t: {p for p in parents_map.get(t, set()) if p in term_to_idx}\n",
    "        for t in classes\n",
    "    }\n",
    "    return restricted_parents\n",
    "restricted_parents = restrict_go_parents(\n",
    "    parents_map,\n",
    "    mlb.classes_\n",
    ")\n",
    "\n",
    "# Propagate GO prediction scores along the hierarchy to enforce the true-path rule\n",
    "def propagate_batch(pred_batch, parents_map, classes, iterations=3):\n",
    "    \n",
    "\n",
    "    term_to_idx = {t: i for i, t in enumerate(classes)}\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        changed = False\n",
    "\n",
    "        for child, parents in parents_map.items():\n",
    "            cidx = term_to_idx[child]\n",
    "            child_scores = pred_batch[:, cidx]\n",
    "\n",
    "            for p in parents:\n",
    "                pidx = term_to_idx[p]\n",
    "                mask = child_scores > pred_batch[:, pidx]\n",
    "                if mask.any():\n",
    "                    pred_batch[mask, pidx] = child_scores[mask]\n",
    "                    changed = True\n",
    "\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    return pred_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156bc073",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "Model Evaluation on Validation Set\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7476844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGB val predict: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [04:26<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== VALIDATION METRICS (Logistic Regression) =====\n",
      "Micro-F1        : 0.1727\n",
      "Macro-F1        : 0.0023\n",
      "Micro-Precision : 0.4166\n",
      "Micro-Recall    : 0.1089\n",
      "Micro PR-AUC    : 0.1094\n",
      "Avg labels/prot : 1.04\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score\n",
    "THRESHOLD = 0.3\n",
    "print(\"Predicting validation...\")\n",
    "val_proba = np.zeros((X_val.shape[0], num_classes))\n",
    "\n",
    "for i in tqdm(range(num_classes), desc=\"XGB val predict\"):\n",
    "    model = models[i]\n",
    "    if model is None:\n",
    "        continue\n",
    "    dval = xgb.DMatrix(X_val)\n",
    "    val_proba[:, i] = model.predict(dval)\n",
    "val_pred = (val_proba >= THRESHOLD).astype(int)\n",
    "# Micro metrics \n",
    "micro_f1 = f1_score(y_val, val_pred, average=\"micro\")\n",
    "micro_precision = precision_score(y_val, val_pred, average=\"micro\", zero_division=0)\n",
    "micro_recall = recall_score(y_val, val_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "# Macro F1 \n",
    "macro_f1 = f1_score(y_val, val_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "# PR-AUC (micro) \n",
    "pr_auc_micro = average_precision_score(y_val, val_proba, average=\"micro\")\n",
    "avg_labels_per_protein = val_pred.sum(axis=1).mean()\n",
    "coverage = (val_pred.sum(axis=1) > 0).mean()\n",
    "print(\"\\n===== VALIDATION METRICS (Logistic Regression) =====\")\n",
    "print(f\"Micro-F1        : {micro_f1:.4f}\")\n",
    "print(f\"Macro-F1        : {macro_f1:.4f}\")\n",
    "print(f\"Micro-Precision : {micro_precision:.4f}\")\n",
    "print(f\"Micro-Recall    : {micro_recall:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d33e61",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "Predict Test Set</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76f0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test in batches...\n",
      "Predicting batch 0:8000\n",
      "Predicting batch 8000:16000\n",
      "Predicting batch 16000:24000\n",
      "Predicting batch 24000:32000\n",
      "Predicting batch 32000:40000\n",
      "Predicting batch 40000:48000\n",
      "Predicting batch 48000:56000\n",
      "Predicting batch 56000:64000\n",
      "Predicting batch 64000:72000\n",
      "Predicting batch 72000:80000\n",
      "Predicting batch 80000:88000\n",
      "Predicting batch 88000:96000\n",
      "Predicting batch 96000:104000\n",
      "Predicting batch 104000:112000\n",
      "Predicting batch 112000:120000\n",
      "Predicting batch 120000:128000\n",
      "Predicting batch 128000:136000\n",
      "Predicting batch 136000:144000\n",
      "Predicting batch 144000:152000\n",
      "Predicting batch 152000:160000\n",
      "Predicting batch 160000:168000\n",
      "Predicting batch 168000:176000\n",
      "Predicting batch 176000:184000\n",
      "Predicting batch 184000:192000\n",
      "Predicting batch 192000:200000\n",
      "Predicting batch 200000:208000\n",
      "Predicting batch 208000:216000\n",
      "Predicting batch 216000:224000\n",
      "Predicting batch 224000:224309\n"
     ]
    }
   ],
   "source": [
    "def xgb_predict_in_batches(models, X, batch_size=10000):\n",
    "    n = X.shape[0]\n",
    "    C = len(models)\n",
    "    proba = np.zeros((n, C), dtype=np.float32)\n",
    "\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "        print(f\"Predicting batch {start}:{end}\")\n",
    "\n",
    "        dmat = xgb.DMatrix(X[start:end])\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            if model is not None:\n",
    "                proba[start:end, i] = model.predict(dmat)\n",
    "\n",
    "        del dmat   # ðŸ”¥ giáº£i phÃ³ng RAM ngay\n",
    "\n",
    "    return proba\n",
    "\n",
    "print(\"Predicting test in batches...\")\n",
    "test_proba = xgb_predict_in_batches(models, X_test, batch_size=8000)\n",
    "test_proba = propagate_batch(\n",
    "    test_proba,\n",
    "    restricted_parents,\n",
    "    mlb.classes_,\n",
    "    iterations=2\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f873a",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "Generate Submission File</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a7512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission2.tsv\n"
     ]
    }
   ],
   "source": [
    "test_ids, test_seqs = read_fasta_ids_and_seqs(TEST_FASTA)\n",
    "\n",
    "out_path = OUT_SUBMISSION\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"EntryID\\tGO_ID\\tConfidence\\n\")\n",
    "    \n",
    "    for i, pid in enumerate(test_ids):\n",
    "        pr = test_proba[i]\n",
    "        idx = np.argsort(-pr)[:TOP_K]  \n",
    "\n",
    "        for j in idx:\n",
    "            score = pr[j]\n",
    "            if score < MIN_PROB:\n",
    "                continue\n",
    "            s = round_sig(score, 3)\n",
    "            if s:\n",
    "                f.write(f\"{pid}\\t{all_terms_sorted[j]}\\t{s}\\n\")\n",
    "\n",
    "print(\"Saved:\", out_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
