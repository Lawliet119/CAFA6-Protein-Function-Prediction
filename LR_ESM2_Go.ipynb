{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e72d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import os, re, shutil, sys\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b79f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FASTA = \"train_sequences.fasta\"\n",
    "TRAIN_TERMS = \"train_terms.tsv\"\n",
    "TRAIN_TAXONOMY = \"train_taxonomy.tsv\"\n",
    "TEST_FASTA  = \"testsuperset.fasta\"\n",
    "TEST_TAXON  = \"testsuperset-taxon-list.tsv\"\n",
    "IA_TSV      = \"IA.tsv\"\n",
    "TRAIN_EMB= \"train_embeddings.npy\"\n",
    "TEST_EMB = \"test_embeddings.npy\"\n",
    "OUT_SUBMISSION = \"submission.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc917c",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "Data Loading\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c7bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training FASTA...\n",
      "Loading training terms...\n",
      "Training instances used: 82404\n",
      "Kept by freq: 1200 | Allowed (IA): 40121\n",
      "Kept after whitelist: 1200\n"
     ]
    }
   ],
   "source": [
    "# Label space control (important for runtime)\n",
    "MIN_COUNT     = 2       # keep GO terms appearing >= 2 times\n",
    "MAX_TERMS     = 1200    # cap number of GO classes \n",
    "\n",
    "# Emission control (evaluator picks the threshold)\n",
    "MIN_PROB      = 1e-6\n",
    "TOP_K         = 200\n",
    "CAP_PER_PROT  = 1500\n",
    "\n",
    "\n",
    "\n",
    "# Training loop controls\n",
    "MIN_POSITIVES_PER_CLASS   = 5            # skip classes with too few positives\n",
    "VAL_SIZE                  = 0.1          \n",
    "RANDOM_STATE              = 42\n",
    "\n",
    "# Read protein IDs and amino acid sequences from a FASTA file\n",
    "def read_fasta_ids_and_seqs(path):\n",
    "    \n",
    "    \n",
    "    ids, seqs, cur_id, cur_seq = [], [], None, []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                if cur_id is not None:\n",
    "                    ids.append(cur_id); seqs.append(\"\".join(cur_seq))\n",
    "                header = line[1:].strip()\n",
    "                cur_id = header.split(\"|\")[1].split()[0] if \"|\" in header else header.split()[0]\n",
    "                cur_seq = []\n",
    "            else:\n",
    "                cur_seq.append(line.strip())\n",
    "        if cur_id is not None:\n",
    "            ids.append(cur_id); seqs.append(\"\".join(cur_seq))\n",
    "    return ids, seqs\n",
    "\n",
    "# Load training GO annotations and standardize column names\n",
    "def load_train_terms(path):\n",
    "    df0 = pd.read_csv(path, sep=\"\\t\", dtype=str)\n",
    "    cols = [c.lower() for c in df0.columns]; df0.columns = cols\n",
    "    if \"entryid\" in cols and \"term\" in cols:\n",
    "        df = df0.rename(columns={\"entryid\":\"protein_id\",\"term\":\"go_id\"})[[\"protein_id\",\"go_id\"]]\n",
    "    else:\n",
    "        df = df0.iloc[:, :2].copy()\n",
    "        df.columns = [\"protein_id\",\"go_id\"]\n",
    "    return df.dropna().drop_duplicates()\n",
    "\n",
    "# Load protein taxonomy information as a dictionary (protein_id -> taxon)\n",
    "def load_taxonomy(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, dtype=str)\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(\"taxonomy TSV must have at least two columns.\")\n",
    "    df = df.iloc[:, :2].copy()\n",
    "    df.columns = [\"protein_id\",\"taxon\"]\n",
    "    return dict(zip(df[\"protein_id\"], df[\"taxon\"]))\n",
    "\n",
    "# Build a mapping from protein IDs to their associated GO term sets\n",
    "def build_label_sets(df_terms):\n",
    "    mp = defaultdict(set)\n",
    "    for p,g in df_terms[[\"protein_id\",\"go_id\"]].itertuples(index=False):\n",
    "        mp[p].add(g)\n",
    "    return mp\n",
    "\n",
    "# Filter GO terms by frequency and optionally limit the total number of terms\n",
    "def filter_terms_by_freq(p2t, min_count=2, max_terms=None):\n",
    "    c = Counter()\n",
    "    for ts in p2t.values(): c.update(ts)\n",
    "    items = [(g,n) for g,n in c.items() if n >= min_count]\n",
    "    items.sort(key=lambda x: x[1], reverse=True)\n",
    "    if max_terms: items = items[:max_terms]\n",
    "    return set(g for g,_ in items)\n",
    "\n",
    "# Load the whitelist of allowed GO terms from an IA TSV file\n",
    "def load_allowed_go_terms(ia_path):\n",
    "    df = pd.read_csv(ia_path, sep=\"\\t\", dtype=str)\n",
    "    go_cols = [c for c in df.columns if \"go\" in c.lower() or \"term\" in c.lower()]\n",
    "    if go_cols:\n",
    "        vals = df[go_cols[0]].dropna().astype(str).tolist()\n",
    "        return {v for v in vals if v.startswith(\"GO:\")}\n",
    "    s = set()\n",
    "    for col in df.columns:\n",
    "        s |= set(df[col].dropna().astype(str))\n",
    "    return {x for x in s if x.startswith(\"GO:\")}\n",
    "\n",
    "# Format prediction scores to a fixed number of significant digits\n",
    "def round_sig(x: float, sig=3) -> str:\n",
    "    if x <= 0: return None\n",
    "    s = f\"{x:.{sig}g}\"\n",
    "    return s if float(s) > 0 else \"0.001\"\n",
    "    \n",
    "print(\"Loading training FASTA...\")\n",
    "train_ids, train_seqs = read_fasta_ids_and_seqs(TRAIN_FASTA)\n",
    "print(\"Loading training terms...\")\n",
    "terms_df = load_train_terms(TRAIN_TERMS)\n",
    "prot2terms_all = build_label_sets(terms_df)\n",
    "\n",
    "X_text, y_terms, used_ids = [], [], []\n",
    "for pid, seq in zip(train_ids, train_seqs):\n",
    "    t = prot2terms_all.get(pid)\n",
    "    if t:\n",
    "        used_ids.append(pid); X_text.append(seq); y_terms.append(sorted(t))\n",
    "print(\"Training instances used:\", len(used_ids))\n",
    "\n",
    "\n",
    "\n",
    "kept_freq = filter_terms_by_freq({pid:set(ts) for pid,ts in zip(used_ids, y_terms)},\n",
    "                                 min_count=MIN_COUNT, max_terms=MAX_TERMS)\n",
    "allowed_terms = load_allowed_go_terms(IA_TSV)\n",
    "print(\"Kept by freq:\", len(kept_freq), \"| Allowed (IA):\", len(allowed_terms))\n",
    "kept_terms = kept_freq & allowed_terms\n",
    "print(\"Kept after whitelist:\", len(kept_terms))\n",
    "y_filtered = [[t for t in ts if t in kept_terms] for ts in y_terms]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d02422f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed embeddings...\n",
      "X_test shape: (224309, 1280)\n"
     ]
    }
   ],
   "source": [
    "#Load embedding features\n",
    "print(\"Loading precomputed embeddings...\")\n",
    "X_train = np.load(TRAIN_EMB)  # shape (N_train, D)\n",
    "X_test  = np.load(TEST_EMB)   # shape (N_test, D)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59bd4f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (82404, 1280)\n",
      "Label matrix shape: (82404, 1200)\n",
      "Num GO classes: 1200\n"
     ]
    }
   ],
   "source": [
    "# Convert multi-label GO annotations into a binary label matrix aligned with ESM2 embeddings\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "all_terms_sorted = sorted(list(kept_terms))\n",
    "mlb = MultiLabelBinarizer(classes=all_terms_sorted)\n",
    "Y = mlb.fit_transform(y_filtered)\n",
    "\n",
    "print(\"Embedding shape:\", X_train.shape)\n",
    "print(\"Label matrix shape:\", Y.shape)\n",
    "print(\"Num GO classes:\", len(all_terms_sorted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae1b5cd",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "Training\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b81b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets with label-aware stratification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, Y, test_size=VAL_SIZE,\n",
    "    random_state=RANDOM_STATE, stratify=(Y.sum(axis=1) > 0)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940aa190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression (1 classifier per GO term)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training classifiers:  96%|█████████▌| 1148/1200 [19:37<00:53,  1.03s/it]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 724. MiB for an array with shape (74163, 1280) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m         clf \u001b[38;5;241m=\u001b[39m LogisticRegression(\n\u001b[0;32m     18\u001b[0m     penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m,       \n\u001b[0;32m     19\u001b[0m     C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,              \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m            \n\u001b[0;32m     23\u001b[0m )\n\u001b[1;32m---> 25\u001b[0m         clf\u001b[38;5;241m.\u001b[39mfit(X_tr, y_i)\n\u001b[0;32m     27\u001b[0m     classifiers\u001b[38;5;241m.\u001b[39mappend(clf)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1223\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1221\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1223\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1224\u001b[0m     X,\n\u001b[0;32m   1225\u001b[0m     y,\n\u001b[0;32m   1226\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1227\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_dtype,\n\u001b[0;32m   1228\u001b[0m     order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1229\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39msolver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1230\u001b[0m )\n\u001b[0;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1302\u001b[0m     X,\n\u001b[0;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1304\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1305\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1306\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1307\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1308\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39mforce_writeable,\n\u001b[0;32m   1309\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1310\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1311\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1312\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1313\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1314\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1316\u001b[0m )\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:751\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    749\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    753\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 724. MiB for an array with shape (74163, 1280) and data type float64"
     ]
    }
   ],
   "source": [
    "# Train One-vs-Rest Logistic Regression classifiers, one binary model per GO term\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Training Logistic Regression (1 classifier per GO term)...\")\n",
    "\n",
    "num_classes = Y.shape[1]\n",
    "classifiers = []\n",
    "train_proba = []\n",
    "\n",
    "for i in tqdm(range(num_classes), desc=\"Training classifiers\"):\n",
    "    y_i = y_tr[:, i]\n",
    "\n",
    "    # Skip nếu class không có positive nào\n",
    "    if y_i.sum() == 0:\n",
    "        clf = None\n",
    "    else:\n",
    "        clf = LogisticRegression(\n",
    "    penalty=\"l2\",       \n",
    "    C=0.5,              \n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=600,\n",
    "    n_jobs=1            \n",
    ")\n",
    "\n",
    "        clf.fit(X_tr, y_i)\n",
    "\n",
    "    classifiers.append(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e051b",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "Hierarchy Constraints\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16fa36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse GO hierarchy from an OBO file and build a mapping from each GO term to its parent terms\n",
    "def load_go_parents(obo_file):\n",
    "    parents_map = {}\n",
    "    current_term = None\n",
    "\n",
    "    with open(obo_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line == \"[Term]\":\n",
    "                current_term = None\n",
    "\n",
    "            elif line.startswith(\"id: GO:\"):\n",
    "                current_term = line.split(\"id: \")[1]\n",
    "                parents_map[current_term] = set()\n",
    "\n",
    "            elif line.startswith(\"is_a:\") and current_term:\n",
    "                parent = line.split(\"is_a: \")[1].split(\" !\")[0]\n",
    "                parents_map[current_term].add(parent)\n",
    "\n",
    "    return parents_map\n",
    "parents_map = load_go_parents(\"go-basic.obo\")\n",
    "\n",
    "# Restrict GO parent relationships to the set of GO terms used by the model\n",
    "def restrict_go_parents(parents_map, classes):\n",
    "    term_to_idx = {t: i for i, t in enumerate(classes)}\n",
    "\n",
    "    restricted_parents = {\n",
    "        t: {p for p in parents_map.get(t, set()) if p in term_to_idx}\n",
    "        for t in classes\n",
    "    }\n",
    "    return restricted_parents\n",
    "restricted_parents = restrict_go_parents(\n",
    "    parents_map,\n",
    "    mlb.classes_\n",
    ")\n",
    "\n",
    "# Propagate GO prediction scores along the hierarchy to enforce the true-path rule\n",
    "def propagate_batch(pred_batch, parents_map, classes, iterations=3):\n",
    "    \n",
    "\n",
    "    term_to_idx = {t: i for i, t in enumerate(classes)}\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        changed = False\n",
    "\n",
    "        for child, parents in parents_map.items():\n",
    "            cidx = term_to_idx[child]\n",
    "            child_scores = pred_batch[:, cidx]\n",
    "\n",
    "            for p in parents:\n",
    "                pidx = term_to_idx[p]\n",
    "                mask = child_scores > pred_batch[:, pidx]\n",
    "                if mask.any():\n",
    "                    pred_batch[mask, pidx] = child_scores[mask]\n",
    "                    changed = True\n",
    "\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    return pred_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8fe40a",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "Model Evaluation on Validation Set\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a782914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting val: 100%|██████████| 1200/1200 [01:10<00:00, 16.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== VALIDATION METRICS (Logistic Regression) =====\n",
      "Micro-F1        : 0.1742\n",
      "Macro-F1        : 0.0016\n",
      "Micro-Precision : 0.4167\n",
      "Micro-Recall    : 0.1101\n",
      "Micro PR-AUC    : 0.1104\n",
      "Avg labels/prot : 1.05\n",
      "Coverage        : 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "\n",
    "val_proba = np.zeros((X_val.shape[0], num_classes))\n",
    "\n",
    "for i in tqdm(range(num_classes), desc=\"Predicting val\"):\n",
    "    clf_i = classifiers[i]\n",
    "    if clf_i is not None:\n",
    "        val_proba[:, i] = clf_i.predict_proba(X_val)[:, 1]\n",
    "val_pred = (val_proba >= THRESHOLD).astype(int)\n",
    "# Micro metrics\n",
    "micro_f1 = f1_score(y_val, val_pred, average=\"micro\")\n",
    "micro_precision = precision_score(y_val, val_pred, average=\"micro\", zero_division=0)\n",
    "micro_recall = recall_score(y_val, val_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "# Macro F1 \n",
    "macro_f1 = f1_score(y_val, val_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "# PR-AUC (micro)\n",
    "pr_auc_micro = average_precision_score(y_val, val_proba, average=\"micro\")\n",
    "avg_labels_per_protein = val_pred.sum(axis=1).mean()\n",
    "coverage = (val_pred.sum(axis=1) > 0).mean()\n",
    "print(\"\\n===== VALIDATION METRICS (Logistic Regression) =====\")\n",
    "print(f\"Micro-F1        : {micro_f1:.4f}\")\n",
    "print(f\"Macro-F1        : {macro_f1:.4f}\")\n",
    "print(f\"Micro-Precision : {micro_precision:.4f}\")\n",
    "print(f\"Micro-Recall    : {micro_recall:.4f}\")\n",
    "print(f\"Micro PR-AUC    : {pr_auc_micro:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f6808",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "Predict Test Set\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc068f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting test: 100%|██████████| 1200/1200 [36:14<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_proba = np.zeros((X_test.shape[0], num_classes))\n",
    "\n",
    "for i in tqdm(range(num_classes), desc=\"Predicting test\"):\n",
    "    clf_i = classifiers[i]\n",
    "    if clf_i is not None:\n",
    "        test_proba[:, i] = clf_i.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# GO HIERARCHY \n",
    "test_proba = propagate_batch(\n",
    "    test_proba,\n",
    "    restricted_parents,\n",
    "    mlb.classes_,\n",
    "    iterations=2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7690b",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "Generate Submission File\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission.tsv\n"
     ]
    }
   ],
   "source": [
    "test_ids, test_seqs = read_fasta_ids_and_seqs(TEST_FASTA)\n",
    "\n",
    "out_path = OUT_SUBMISSION\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"EntryID\\tGO_ID\\tConfidence\\n\")\n",
    "    \n",
    "    for i, pid in enumerate(test_ids):\n",
    "        pr = test_proba[i]\n",
    "        idx = np.argsort(-pr)[:TOP_K] \n",
    "\n",
    "        for j in idx:\n",
    "            score = pr[j]\n",
    "            if score < MIN_PROB:\n",
    "                continue\n",
    "            s = round_sig(score, 3)\n",
    "            if s:\n",
    "                f.write(f\"{pid}\\t{all_terms_sorted[j]}\\t{s}\\n\")\n",
    "\n",
    "print(\"Saved:\", out_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
